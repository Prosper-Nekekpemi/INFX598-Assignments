{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "635a5e75",
   "metadata": {},
   "source": [
    "### Question 1: What is the approximate depth of a Decision Tree trained (without restrictions) on a training set with one million instances?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca62e6f",
   "metadata": {},
   "source": [
    "Answer: The depth of a well-balanced binary tree containing m leaves is equal to log2(m),2 rounded up. Thus, if the training set contains one million instances, the Decision Tree will have a depth of log2(106) ≈ 20 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483f3b55",
   "metadata": {},
   "source": [
    "### Question 3: If a Decision Tree is overfitting the training set, is it a good idea to try decreasing max_depth?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191cd0b2",
   "metadata": {},
   "source": [
    "Answer: It may be a good idea to decrease max_depth when the Decision Tree is overfitting the training set since this will constrain the model, regularizing it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6a1dbe",
   "metadata": {},
   "source": [
    "### Question 4: If a Decision Tree is underfitting the training set, is it a good idea to try scaling the input features?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282fc2e5",
   "metadata": {},
   "source": [
    "Answer: If a Decision Tree is underfitting the training set, scaling the input features will be a waste of time because the Decision Trees don't care whether or not the training set is scaled or centered."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42881dd2",
   "metadata": {},
   "source": [
    "### Question 7: Train and fine-tune a Decision Tree for the moons dataset by following these steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7735f8ee",
   "metadata": {},
   "source": [
    "a. Use make_moons(n_samples=10000, noise=0.4) to generate a moons dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b4d20b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the dataset\n",
    "\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "X, y = make_moons(n_samples = 10000, noise = 0.4, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6b7bc9",
   "metadata": {},
   "source": [
    "b. Use train_test_split() to split the dataset into a training set and a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f4b05b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45667762",
   "metadata": {},
   "source": [
    "c. Use grid search with cross-validation (with the help of the GridSearchCV class) to find good hyperparameter values for a DecisionTreeClassifier. Hint: try various values for max_leaf_nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc55413e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 294 candidates, totalling 882 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, estimator=DecisionTreeClassifier(random_state=42),\n",
       "             param_grid={'max_leaf_nodes': [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,\n",
       "                                            13, 14, 15, 16, 17, 18, 19, 20, 21,\n",
       "                                            22, 23, 24, 25, 26, 27, 28, 29, 30,\n",
       "                                            31, ...],\n",
       "                         'min_samples_split': [2, 3, 4]},\n",
       "             verbose=1)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dct_clf = DecisionTreeClassifier(random_state = 42)\n",
    "\n",
    "params = {'max_leaf_nodes': list(range(2,100)),\n",
    "         'min_samples_split': [2,3,4]}\n",
    "grid_search = GridSearchCV(dct_clf, params, verbose=1, cv=3 )\n",
    "\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3f56afa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(max_leaf_nodes=17, random_state=42)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2caddc5e",
   "metadata": {},
   "source": [
    "d. Train it on the full training set using these hyperparameters, and measure your model’s performance on the test set. You should get roughly 85% to 87% accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7130b549",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8695"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = grid_search.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7dde10b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
